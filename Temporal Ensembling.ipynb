{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from timeit import default_timer as timer\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils import weight_norm\n",
    "\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    \n",
    "    transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    \n",
    "    train_set = datasets.MNIST(root = \"../data\",\n",
    "                               transform = transform,\n",
    "                               train = True,\n",
    "                               download = True)\n",
    "    test_set = datasets.MNIST(root = \"../data\",\n",
    "                              transform = transform,\n",
    "                              train = False)\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GaussianNoise(nn.Module):\n",
    "    def __init__(self, batch_size=100, input_shape = (1,28,28),std=0.15):\n",
    "        super(GaussianNoise, self).__init__()\n",
    "        self.shape = (batch_size,) + input_shape\n",
    "        self.noise = Variable(torch.zeros(self.shape).cuda())\n",
    "        self.std = std\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.noise.data.normal_(0, std=self.std)\n",
    "        return x + self.noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.GaussianNoise = GaussianNoise()\n",
    "        self.conv = torch.nn.Sequential(torch.nn.Conv2d(1, 16, 3, padding=1),\n",
    "                                         torch.nn.MaxPool2d(3, stride=2, padding=1),\n",
    "                                         torch.nn.ReLU(),\n",
    "                                         torch.nn.Conv2d(16, 32, 3, padding=1),\n",
    "                                         torch.nn.MaxPool2d(3, stride=2, padding=1),\n",
    "                                         torch.nn.ReLU())\n",
    "        self.dense = torch.nn.Sequential(torch.nn.Dropout(p=0.5),\n",
    "                                         torch.nn.Linear(32 * 7 * 7, 10))\n",
    "    def forward(self, x):\n",
    "        if(self.training):\n",
    "            x = self.GaussianNoise(x)\n",
    "        x = self.conv(x)\n",
    "        x = x.view(-1, 32 * 7 * 7)\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ramp_up(epoch, max_epochs, max_val, mult):\n",
    "    if epoch == 0:\n",
    "        return 0.\n",
    "    elif epoch >= max_epochs:\n",
    "        return max_val\n",
    "    return max_val * np.exp(mult * (1. - float(epoch) / max_epochs) ** 2)\n",
    "\n",
    "def weight_schedule(epoch, max_epochs, max_val, mult, n_labeled, n_samples):\n",
    "    max_val = max_val * (float(n_labeled) / n_samples)\n",
    "    return ramp_up(epoch, max_epochs, max_val, mult)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_train(train_dataset, test_dataset, batch_size, n_labels, n_classes):\n",
    "    \n",
    "    n = len(train_dataset)\n",
    "    \n",
    "    random.seed(5)\n",
    "    \n",
    "    cnt = 0\n",
    "    labels_index = torch.zeros(n_labels)\n",
    "    unlabel_index = torch.zeros(n - n_labels)\n",
    "    labels_class = n_labels // n_classes # num of labeled instances in each class\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        class_items = (train_dataset.targets == i).nonzero()[:,0]\n",
    "        n_class = len(class_items) # num instances in this class\n",
    "        rand_index = np.random.permutation(np.arange(n_class)) # pertub the index\n",
    "        labels_index[i * labels_class : (i + 1) * labels_class] = class_items[rand_index[:labels_class]]\n",
    "        unlabel_index[cnt:cnt+n_class-labels_class] = class_items[rand_index[labels_class:]]\n",
    "        cnt += n_class - labels_class\n",
    "    \n",
    "    unlabel_index = unlabel_index.long() #tensors used as indices must be long or byte tensors\n",
    "\n",
    "    train_dataset.targets[unlabel_index] = -1\n",
    "    train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                              batch_size = batch_size,\n",
    "                                              shuffle = False)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                              batch_size = batch_size,\n",
    "                                              shuffle = False)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def temporal_loss(out1, out2, w, labels):\n",
    "    #Supervised Loss\n",
    "    def mse_loss(out1, out2):\n",
    "        quad_diff = torch.sum((F.softmax(out1, dim=1) - F.softmax(out2, dim=1)) ** 2)\n",
    "        return quad_diff / out1.data.nelement()\n",
    "    \n",
    "    def masked_crossentropy(out, labels):\n",
    "        labeled_index = (labels >= 0)\n",
    "        nnz = torch.nonzero(labeled_index)\n",
    "        num_labeled = len(nnz)\n",
    "        \n",
    "        #check if labeled samples in batch, return 0 if none\n",
    "        if num_labeled > 0:\n",
    "            masked_outputs = torch.index_select(out, 0, nnz.view(num_labeled))\n",
    "            masked_labels = labels[labeled_index]                                 \n",
    "            loss = F.cross_entropy(masked_outputs, masked_labels)\n",
    "            return loss, num_labeled\n",
    "        return Variable(torch.FloatTensor([0.]).cuda(), requires_grad=False), 0\n",
    "    \n",
    "    sup_loss, num_labeled = masked_crossentropy(out1, labels)\n",
    "    unsup_loss = mse_loss(out1, out2)\n",
    "    return sup_loss + w * unsup_loss, sup_loss, unsup_loss, num_labeled\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, n_labels = 100, n_epochs=300, batch_size=100,\n",
    "          max_epochs=80, max_val=30., ramp_up_mult=-5.,\n",
    "          n_classes=10,n_samples=60000, alpha=0.6, learning_rate=0.002):\n",
    "    \n",
    "    train_dataset, test_dataset = load_mnist()\n",
    "    n_train = len(train_dataset)\n",
    "    \n",
    "    #build model\n",
    "    model.cuda()\n",
    "    \n",
    "    train_loader, test_loader = sample_train(train_dataset, test_dataset, batch_size,\n",
    "                                            n_labels, n_classes)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.99))\n",
    "    \n",
    "    #train\n",
    "    model.train()\n",
    "    losses = []\n",
    "    supvised_loss = []\n",
    "    unsupvised_loss = []\n",
    "    \n",
    "    \n",
    "    Z = torch.zeros(n_train, n_classes).float().cuda()\n",
    "    z = torch.zeros(n_train, n_classes).float().cuda()\n",
    "    outputs = torch.zeros(n_train, n_classes).float().cuda()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        #unsupervised loss weight\n",
    "        w = weight_schedule(epoch, max_epochs, max_val, ramp_up_mult, n_labels, n_samples)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print('Unsupervised loss weight: {}'.format(w))\n",
    "            \n",
    "        w = torch.autograd.Variable(torch.FloatTensor([w]).cuda(), requires_grad=False)\n",
    "        \n",
    "        l = []\n",
    "        sup_l = []\n",
    "        unsup_l = []\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            \n",
    "            t = timer()\n",
    "            images = Variable(images.cuda())\n",
    "            labels = Variable(labels.cuda(), requires_grad=False)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            out = model(images)\n",
    "            \n",
    "            zcomp = Variable(z[i * batch_size: (i+1)*batch_size], requires_grad=False)\n",
    "            loss, sup_loss, unsup_loss, num_sup = temporal_loss(out, zcomp, w, labels)\n",
    "            \n",
    "            #save outputs and losses\n",
    "            outputs[i * batch_size: (i+1) * batch_size] = out.data.clone()\n",
    "            l.append(loss.item())\n",
    "            sup_l.append(num_sup * sup_loss.item())\n",
    "            unsup_l.append(unsup_loss.item())\n",
    "            \n",
    "            #backprop\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                if i + 1 == 2 * 300:\n",
    "                    print ('Epoch [%d/%d], Step [%d/%d], Loss: %.6f, Time (this epoch): %.2f s' \n",
    "                           %(epoch + 1, n_epochs, i + 1, len(train_dataset) // batch_size, np.mean(l), timer() - t))\n",
    "                elif (i + 1) % 300 == 0:\n",
    "                    print ('Epoch [%d/%d], Step [%d/%d], Loss: %.6f' \n",
    "                           %(epoch + 1, n_epochs, i + 1, len(train_dataset) // batch_size, np.mean(l)))\n",
    "\n",
    "                \n",
    "                \n",
    "        Z = alpha * Z + (1. - alpha) * outputs\n",
    "        z = Z * (1. / (1. - alpha ** (epoch + 1)))\n",
    "        \n",
    "        #handle metrics, losses, etc\n",
    "        ave_loss = np.mean(l)\n",
    "        losses.append(ave_loss)\n",
    "        supvised_loss.append((1. / n_labels) * np.sum(sup_l))\n",
    "        unsupvised_loss.append(np.mean(unsup_l))\n",
    "        \n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, (samples, labels) in enumerate(test_loader):\n",
    "            samples = Variable(samples.cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "            out = model(samples)\n",
    "            _,pred = torch.max(out.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (pred == labels.data).sum()\n",
    "        print(\"Test Accuracy: %.4f\", 1.0 * correct.item() / total)\n",
    "    return losses, supvised_loss, unsupvised_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsupervised loss weight: 0.0009740835053472244\n",
      "Epoch [10/300], Step [300/600], Loss: 0.098268\n",
      "Epoch [10/300], Step [600/600], Loss: 0.070119, Time (this epoch): 0.01 s\n",
      "Unsupervised loss weight: 0.0027318847495129677\n",
      "Epoch [20/300], Step [300/600], Loss: 0.024207\n",
      "Epoch [20/300], Step [600/600], Loss: 0.044935, Time (this epoch): 0.01 s\n",
      "Unsupervised loss weight: 0.0065534508315721485\n",
      "Epoch [30/300], Step [300/600], Loss: 0.014359\n",
      "Epoch [30/300], Step [600/600], Loss: 0.007275, Time (this epoch): 0.01 s\n",
      "Unsupervised loss weight: 0.013446808012990042\n",
      "Epoch [40/300], Step [300/600], Loss: 0.120434\n",
      "Epoch [40/300], Step [600/600], Loss: 0.060298, Time (this epoch): 0.01 s\n",
      "Unsupervised loss weight: 0.023599883024449233\n",
      "Epoch [50/300], Step [300/600], Loss: 0.000698\n",
      "Epoch [50/300], Step [600/600], Loss: 0.053739, Time (this epoch): 0.01 s\n",
      "Unsupervised loss weight: 0.035427620478902945\n",
      "Epoch [60/300], Step [300/600], Loss: 0.051736\n",
      "Epoch [60/300], Step [600/600], Loss: 0.026030, Time (this epoch): 0.01 s\n",
      "Unsupervised loss weight: 0.04548996514767851\n",
      "Epoch [70/300], Step [300/600], Loss: 0.124953\n",
      "Epoch [70/300], Step [600/600], Loss: 0.064802, Time (this epoch): 0.01 s\n",
      "Unsupervised loss weight: 0.0499609527548162\n",
      "Epoch [80/300], Step [300/600], Loss: 0.000427\n",
      "Epoch [80/300], Step [600/600], Loss: 0.000426, Time (this epoch): 0.01 s\n",
      "Unsupervised loss weight: 0.05\n",
      "Epoch [90/300], Step [300/600], Loss: 0.145151\n",
      "Epoch [90/300], Step [600/600], Loss: 0.072793, Time (this epoch): 0.01 s\n",
      "Unsupervised loss weight: 0.05\n",
      "Epoch [100/300], Step [300/600], Loss: 0.011626\n",
      "Epoch [100/300], Step [600/600], Loss: 0.078978, Time (this epoch): 0.01 s\n",
      "Unsupervised loss weight: 0.05\n",
      "Epoch [110/300], Step [300/600], Loss: 0.000424\n",
      "Epoch [110/300], Step [600/600], Loss: 0.020977, Time (this epoch): 0.01 s\n",
      "Unsupervised loss weight: 0.05\n",
      "Epoch [120/300], Step [300/600], Loss: 0.000401\n",
      "Epoch [120/300], Step [600/600], Loss: 0.024957, Time (this epoch): 0.01 s\n",
      "Unsupervised loss weight: 0.05\n",
      "Epoch [130/300], Step [300/600], Loss: 0.000377\n",
      "Epoch [130/300], Step [600/600], Loss: 0.000361, Time (this epoch): 0.01 s\n",
      "Unsupervised loss weight: 0.05\n",
      "Epoch [140/300], Step [300/600], Loss: 0.000380\n",
      "Epoch [140/300], Step [600/600], Loss: 0.122820, Time (this epoch): 0.01 s\n",
      "Unsupervised loss weight: 0.05\n",
      "Epoch [150/300], Step [300/600], Loss: 0.043041\n",
      "Epoch [150/300], Step [600/600], Loss: 0.110457, Time (this epoch): 0.01 s\n",
      "Unsupervised loss weight: 0.05\n",
      "Epoch [160/300], Step [300/600], Loss: 0.000454\n",
      "Epoch [160/300], Step [600/600], Loss: 0.017061, Time (this epoch): 0.01 s\n",
      "Unsupervised loss weight: 0.05\n",
      "Epoch [170/300], Step [300/600], Loss: 0.000383\n",
      "Epoch [170/300], Step [600/600], Loss: 0.000372, Time (this epoch): 0.01 s\n",
      "Unsupervised loss weight: 0.05\n",
      "Epoch [180/300], Step [300/600], Loss: 0.000369\n",
      "Epoch [180/300], Step [600/600], Loss: 0.000364, Time (this epoch): 0.01 s\n",
      "Unsupervised loss weight: 0.05\n",
      "Epoch [190/300], Step [300/600], Loss: 0.274526\n",
      "Epoch [190/300], Step [600/600], Loss: 0.170635, Time (this epoch): 0.01 s\n",
      "Unsupervised loss weight: 0.05\n",
      "Epoch [200/300], Step [300/600], Loss: 0.000321\n",
      "Epoch [200/300], Step [600/600], Loss: 0.000313, Time (this epoch): 0.01 s\n",
      "Unsupervised loss weight: 0.05\n",
      "Epoch [210/300], Step [300/600], Loss: 0.000268\n",
      "Epoch [210/300], Step [600/600], Loss: 0.000264, Time (this epoch): 0.01 s\n",
      "Unsupervised loss weight: 0.05\n",
      "Epoch [220/300], Step [300/600], Loss: 0.000312\n",
      "Epoch [220/300], Step [600/600], Loss: 0.029143, Time (this epoch): 0.01 s\n",
      "Unsupervised loss weight: 0.05\n",
      "Epoch [230/300], Step [300/600], Loss: 0.069212\n",
      "Epoch [230/300], Step [600/600], Loss: 0.034757, Time (this epoch): 0.01 s\n",
      "Unsupervised loss weight: 0.05\n",
      "Epoch [240/300], Step [300/600], Loss: 0.189437\n",
      "Epoch [240/300], Step [600/600], Loss: 0.104629, Time (this epoch): 0.01 s\n",
      "Unsupervised loss weight: 0.05\n",
      "Epoch [250/300], Step [300/600], Loss: 0.000258\n",
      "Epoch [250/300], Step [600/600], Loss: 0.062228, Time (this epoch): 0.01 s\n",
      "Unsupervised loss weight: 0.05\n",
      "Epoch [260/300], Step [300/600], Loss: 0.000294\n",
      "Epoch [260/300], Step [600/600], Loss: 0.000291, Time (this epoch): 0.01 s\n",
      "Unsupervised loss weight: 0.05\n",
      "Epoch [270/300], Step [300/600], Loss: 0.000246\n",
      "Epoch [270/300], Step [600/600], Loss: 0.000243, Time (this epoch): 0.01 s\n",
      "Unsupervised loss weight: 0.05\n",
      "Epoch [280/300], Step [300/600], Loss: 0.000231\n",
      "Epoch [280/300], Step [600/600], Loss: 0.000232, Time (this epoch): 0.01 s\n",
      "Unsupervised loss weight: 0.05\n",
      "Epoch [290/300], Step [300/600], Loss: 0.000234\n",
      "Epoch [290/300], Step [600/600], Loss: 0.000235, Time (this epoch): 0.01 s\n",
      "Unsupervised loss weight: 0.05\n",
      "Epoch [300/300], Step [300/600], Loss: 0.000256\n",
      "Epoch [300/300], Step [600/600], Loss: 0.000254, Time (this epoch): 0.01 s\n",
      "Test Accuracy: %.4f 0.9776\n"
     ]
    }
   ],
   "source": [
    "model = CNN()\n",
    "losses, supervised_loss, unsupervised_loss = train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
